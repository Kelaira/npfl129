title: NPFL129, Lecture 4
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Multilayer Perceptron

## Milan Straka

### October 26, 2020

---
section: Refresh
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|→x)$ and of $p(C_1|→x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | →x) &= σ(→x^t →w + b) \\
  p(C_0 | →x) &= 1 - p(C_1 | →x),
\end{aligned}$$

![w=60%,f=right](../03/sigmoid.svgz)

where $σ$ is a **sigmoid function**
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$p(C_1 | →x) = σ(y(→x; →w)) = \frac{1}{1 + e^{-y(→x; →w)}}$$
~~~
we can arrive at
$$y(→x; →w) = \log\left(\frac{p(C_1 | →x)}{p(C_0 | →x)}\right),$$
where the prediction of the model $y(→x; →w)$ is called a **logit**
and it is a logarithm of odds of the two classes probabilities.

---
# Logistic Regression

To train the logistic regression $y(→x; →w) = →x^T →w$, we use MLE (the maximum likelihood
estimation). Note that $p(C_1 | →x; →w) = σ(y(→x; →w))$.

~~~
Therefore, the loss for a batch $𝕏=\{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$
is
$$\begin{aligned}
𝓛(𝕏) = \frac{1}{N} ∑_i -\log(p(C_{t_i} | →x_i; →w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, +1\}^N$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(p(C_{t_i} | →x_i; →w)$
  - $→w ← →w - α→g$
</div>

---
# Linearity in Logistic Regression

![w=100%](lr_linearity.svgz)

---
section: GLM
class: tablewide
# Generalized Linear Models

The logistic regression is in fact an extended linear regression. A linear
regression model, which is followed by some **activation function** $a$, is
called **generalized linear model**:
$$p(t | →x; →w, b) = a\big(y(→x; →w, b)\big) = a(x^T→w + b).$$

~~~
| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | ? | $\textrm{MSE} ∝ ∑ (t - y(→x))^2$ | $\big(t - y(→x)\big) ⋅ →x$ |
~~~
| logistic regression | $σ(x)$ | Bernoulli | $\textrm{NLL} ∝ ∑ -\log(p(t \vert →x))$ | ? |
~~~ ~~
| logistic regression | $σ(x)$ | Bernoulli | $\textrm{NLL} ∝ ∑ -\log(p(t \vert →x))$ | $\color{red}\big(t - a(y(→x))\big) ⋅ →x$ |

---
section: MSE as MLE
# Mean Square Error as MLE

During regression, we predict a number, not a real probability distribution.
In order to generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $σ^2$ – the most general
such a distribution is the normal distribution.

---
# Mean Square Error as MLE

Therefore, assume our model generates a distribution
$p(t | →x; →w) = 𝓝(t; y(→x; →w), σ^2)$.

~~~
Now we can apply MLE and get
$$\begin{aligned}
\argmax_→w p(𝕏; →w) =& \argmin_→w ∑_{i=1}^N -\log p(t_i | →x_i ; →w) \\
                         =& -\argmin_→w ∑_{i=1}^N \log \sqrt{\frac{1}{2πσ^2}}
                            e ^ {\normalsize -\frac{(t_i - y(→x_i; →w))^2}{2σ^2}} \\
                         =& -\argmin_→w {\color{gray} N \log (2πσ^2)^{-1/2} +}
                            ∑_{i=1}^N -\frac{(t_i - y(→x_i; →w))^2}{2σ^2} \\
                         =& \argmin_→w ∑_{i=1}^N \frac{(t_i - y(→x_i; →w))^2}{2σ^2} = \argmin_→w \tfrac{1}{N} ∑_{i=1}^N (t_i - y(→x_i; →w))^2.
\end{aligned}$$

---
class: tablewide
# Generalized Linear Models

We have therefore extended the GLM table to

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | $\color{red}\textrm{Normal}$ | $\color{red}\textrm{NLL} ∝ \textrm{MSE}$ | $\big(t - y(→x)\big) ⋅ →x$ |
| logistic regression | $σ(x)$ | Bernoulli | $\textrm{NLL} ∝ ∑_i -\log(p(t \vert →x))$ | $\big(t - a(y(→x))\big) ⋅ →x$ |

---
section: MulticlassLogisticReg
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- generate multiple outputs, notably $K$ outputs, each with its own set of
  weights, so that
  $$→y(→x; ⇉W) = ⇉W →x,\textrm{~~~or in other words~~~}→y(→x; ⇉W)_i = →W_i^T →x,$$

~~~
- generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(→y)_i = \frac{e^{y_i}}{∑_j e^{y_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$σ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as **multinomial logistic regression**,
**maximum entropy classifier** or **softmax regression**.

---
# Multiclass Logistic Regression

From the definition of the $\softmax$ function
$$\softmax(→y)_i = \frac{e^{y_i}}{∑_j e^{y_j}},$$
it is natural to obtain the interpretation of the model outputs
$→y(→x; ⇉W)$ as **logits**:
$$→y(→x; ⇉W)_i = \log(p(C_i | →x; ⇉W)) + c.$$

~~~
The constant $c$ is present, because the output of the model is
_overparametrized_ (the probability of for example the last class could be
computed from the remaining ones). This is connected to the fact that softmax
is invariant to addition of a constant:
$$\softmax(→y + c)_i = \frac{e^{y_i + c}}{∑_j e^{y_j + c}} = \frac{e^{y_i}}{∑_j e^{y_j}}⋅\frac{e^c}{e^c} = \softmax(→y)_i.$$

---
# Multiclass Logistic Regression

The difference between softmax and sigmoid output can be compared on the binary
case, where the binary logistic regression model outputs are
$$y(→x; →w) = \log\left(\frac{p(C_1 | →x; →w)}{p(C_0 | →x; →w)}\right),$$
while the outputs of the softmax variant with two outputs can be interpreted as<br>
$→y(→x; ⇉W)_0 = \log(p(C_0 | →x; ⇉W)) + c$ and $→y(→x; ⇉W)_1 = \log(p(C_1 | →x; ⇉W)) + c$.

~~~
If we consider $→y(→x; ⇉W)_0$ to be zero, the model can then predict only the
probability $p(C_1 | →x)$, and the constant $c$ is fixed to $-\log(p(C_0 | →x; ⇉W))$, recovering
the original interpretation.

~~~
Therefore, we could produce only $K-1$ outputs for $K$-class classification and
define $y_K=0$, resulting in the interpretation of the model outputs analogous to the
binary case:

$$→y(→x; ⇉W)_i = \log\left(\frac{p(C_i | →x; ⇉W)}{p(C_K | →x; ⇉W)}\right).$$

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$p(C_i | →x; ⇉W) = \softmax(⇉W_i^T →x)_i = \frac{e^{⇉W_i^T →x}}{∑_j e^{⇉W_j^T →x}}.$$

~~~
We can then use MLE and train the model using stochastic gradient descent.

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, 1, …, K-1\}^N$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(p(C_{t_i} | →x_i; →w)$
  - $→w ← →w - α→g$
</div>

---
# Multiclass Logistic Regression

![w=41%,f=right](classification_convex_regions.svgz)

Note that the decision regions of the binary/multiclass logistic regression are
convex (and therefore connected).

~~~
To see this, consider $→x_A$ and $→x_B$ in the same decision region $R_k$.

~~~
Any point $→x$ lying on the line connecting them is their linear combination,
$→x = λ→x_A + (1-λ)→x_B$,
~~~
and from the linearity of $→y(→x) = ⇉W→x$ it follows that
$$→y(→x) = λ→y(→x_A) + (1-λ)→y(→x_B).$$

~~~
Given that $f_k(→x_A)$ was the largest among $→y(→x_A)$ and also
given that $f_k(→x_B)$ was the largest among $→y(→x_B)$, it must
be the case that $f_k(→x)$ is the largest among all $→y(→x)$.

---
class: tablewide
# Generalized Linear Models

The multiclass logistic regression can now be added to the GLM table:

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | Normal | $\textrm{NLL} ∝ \textrm{MSE}$ | $\big(t - y(→x)\big) ⋅ →x$ |
| logistic regression | $σ(x)$ | Bernoulli | $\textrm{NLL} ∝ ∑ -\log(p(t \vert →x))$ | $\big(t - a(y(→x))\big) ⋅ →x$ |
| multiclass<br>logistic regression | $\small\operatorname{softmax}(x)$ | categorical | $\textrm{NLL} ∝ ∑ -\log(p(t \vert →x))$ | $\big(t - a(y(→x))\big) ⋅ →x$ |

---
section: PoissonReg
class: tablewide
# Poisson Regression

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | Normal | $\textrm{NLL} ∝ \textrm{MSE}$ | $\big(t - y(→x)\big) ⋅ →x$ |
| logistic regression | $σ(x)$ | Bernoulli | $\textrm{NLL} ∝ ∑ -\log(p(t \vert →x))$ | $\big(t - a(y(→x))\big) ⋅ →x$ |
| multiclass<br>logistic regression | $\small\operatorname{softmax}(x)$ | categorical | $\textrm{NLL} ∝ ∑ -\log(p(t \vert →x))$ | $\big(t - a(y(→x))\big) ⋅ →x$ |
| Poisson regression | $\small\exp(x)$ | Poisson | $\textrm{NLL} ∝ ∑ -\log(p(t \vert →x))$ | $\big(t - a(y(→x))\big) ⋅ →x$ |


---
section: MLP
# Multilayer Perceptron

![w=45%,h=center](mlp.svgz)

---
# Multilayer Perceptron

There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on the output layer.
$$h_i = f\left(∑_j w_{i,j} x_j + b_i\right)$$

If the network is composed of layers, we can use matrix notation and write:
$$→h = f\left(⇉W →x + →b\right)$$

---
# Multilayer Perceptron and Biases

![w=55%,h=center](mlp_bias.svgz)

---
# Neural Network Activation Functions
## Output Layers
- none (linear regression if there are no hidden layers)

~~~
- sigmoid (logistic regression model if there are no hidden layers)
  $$σ(x) ≝ \frac{1}{1 + e^{-x}}$$

~~~
- $\softmax$ (maximum entropy model if there are no hidden layers)
  $$\softmax(→x) ∝ e^→x$$
  $$\softmax(→x)_i ≝ \frac{e^{x_i}}{∑_j e^{x_j}}$$

---
# Neural Network Activation Functions
## Hidden Layers
- none (does not help, composition of linear mapping is a linear mapping)

~~~
- $σ$ (but works badly – nonsymmetrical, $\frac{dσ}{dx}(0) = 1/4$)

~~~
- $\tanh$
    - result of making $σ$ symmetrical and making derivation in zero 1
    - $\tanh(x) = 2σ(2x) - 1$

~~~
- ReLU
    - $\max(0, x)$

---
# Training MLP

The multilayer perceptron can be trained using an SGD algorithm:

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t$ targets), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← ∇_→w \frac{1}{N} ∑_j - \log p(t_j | →x_j; →w)$
  - $→w ← →w - α→g$
</div>

---
# Training MLP – Computing the Derivatives

![w=20%,f=right](mlp.svgz)

Assume a network with an input of size $N_1$, then weights
$→U ∈ ℝ^{N_1 × N_2}$, hidden layer with size $N_2$ and activation $h$,
weights $→V ∈ ℝ^{N_2 × N_3}$, and finally an output layer of size $N_3$
with activation $a$.

~~~
In order to compute the gradient of the loss $L$ with respect to all weights, you
should proceed gradually:
- first compute $\frac{∂L}{∂→o}$,

~~~
- then compute $\frac{∂→o}{∂→o_\mathit{in}}$, where $o_\mathit{in}$ are the
  inputs to the output layer (i.e., before applying activation function $f$),
~~~
- then compute $\frac{∂→o_\mathit{in}}{∂⇉V}$ and $\frac{∂L}{∂⇉V}$,
~~~
- followed by $\frac{∂→o_\mathit{in}}{∂→h}$ and $\frac{∂→h}{∂→h_\mathit{in}}$,
~~~
- and finally using $\frac{∂→h_\mathit{in}}{∂⇉U}$ to compute $\frac{∂L}{∂⇉U}$.

---
section: UniversalApproximation
# Universal Approximation Theorem '89

Let $φ(x)$ be a nonconstant, bounded and nondecreasing continuous function.
<br>(Later a proof was given also for $φ = \ReLU$.)

Then for any $ε > 0$ and any continuous function $f$ on $[0, 1]^m$ there exists
an $N ∈ ℕ, v_i ∈ ℝ, b_i ∈ ℝ$ and $→{w_i} ∈ ℝ^m$, such that if we denote
$$F(→x) = ∑_{i=1}^N v_i φ(→{w_i} \cdot →x + b_i),$$
then for all $x ∈ [0, 1]^m$:
$$|F(→x) - f(→x)| < ε.$$

---
class: dbend
# Universal Approximation Theorem for ReLUs

Sketch of the proof:

~~~
- If a function is continuous on a closed interval, it can be approximated by
  a sequence of lines to arbitrary precision.

![w=50%,h=center](universal_approximation_example.png)

~~~
- However, we can create a sequence of $k$ linear segments as a sum of $k$ ReLU
  units – on every endpoint a new ReLU starts (i.e., the input ReLU value is
  zero at the endpoint), with a tangent which is the difference between the
  target tanget and the tangent of the approximation until this point.

---
class: dbend
# Universal Approximation Theorem for Squashes

Sketch of the proof for a squashing function $φ(x)$ (i.e., nonconstant, bounded and
nondecreasing continuous function like sigmoid):

~~~
- We can prove $φ$ can be arbitrarily close to a hard threshold by compressing
  it horizontally.

![w=38%,h=center](universal_approximation_squash.png)

~~~
- Then we approximate the original function using a series of straight line
  segments

![w=38%,h=center](universal_approximation_rectangles.png)
