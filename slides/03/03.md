title: NPFL129, Lecture 3
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Milan Straka

### October 19, 2020

---
section: CV
# Cross-Validation

We already talked about a **train set** and a **test set**. Given that the main
goal of machine learning is to perform well on unseen data, the test set must
not be used during training nor hyperparameter selection. Ideally, it is hidden
to us altogether.

~~~
Therefore, to evaluate a machine learning model (for example to select model
architecture, features, or hyperparameter value), we normally need the
**validation** or a **development** set.

~~~
However, using a single development set might give us noisy results. To obtain
less noisy results (i.e., with smaller variance), we can use
**cross-validation**.

~~~
![w=48%,f=right](k-fold_cross_validation.svgz)

In cross-validation, we choose multiple validation sets from the training data,
and for every one, we train a model on the rest of the training data and
evaluate on the chosen validation sets. A commonly used strategy to choose
the validation sets is called **k-fold cross-validation**. Here the training set is partitioned
into $k$ subsets of approximately the same size, and each subset takes turn
to play a role of a validation set.

---
# Cross-Validation

An extreme case of the **k-fold cross-validation** is **leave-one-out
cross-validation**, where every element is considered a separate validation
set.

~~~
Computing leave-one-out cross-validation is usually extremely inefficient for
larger training sets, but in case of linear regression it can be evaluated
efficiently.
